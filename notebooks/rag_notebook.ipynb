{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca43c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex\n",
    "from llama_index.core.schema import Document\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import openai\n",
    "import os\n",
    "\n",
    "import json\n",
    "with open(\"../keys/keys.json\", \"r\") as fi:\n",
    "    api_key = json.load(fi)['api_key']\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43944ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lexil\\\\Documents\\\\NSS_Projects\\\\nlp-06-rag-Jorgen85Lex\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6b2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2505.23724v1  [cs.LG]  29 May 2025SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n",
      "Subspace-Constrained LoRA\n",
      "Minrui Luo∗1,2, Fuhang Kuang∗1, Yu Wang3, Zirui Liu1, Tianxing He†1,2\n",
      "1Institute for Interdisciplinary Information Sciences, Tsinghua University\n",
      "2Shanghai Qi Zhi Institute\n",
      "3Institute of Information Engineering, Chinese Academy of Sciences\n",
      "{luomr22,kfh22,liu-zr22}@mails.tsinghua.edu.cn;\n",
      "wangyu2002@iie.ac.cn hetianxing@mail.tsinghua.edu.cn\n",
      "Abstract\n",
      "Parameter-Efficient Fine-Tuning (PEFT)\n",
      "methods, particularly Low-Rank Adaptation\n",
      "(LoRA), are indispensable for efficiently\n",
      "customizing Large Language Models (LLMs).\n",
      "However, vanilla LoRA suffers from slow\n",
      "convergence speed and knowledge forgetting\n",
      "problems. Recent studies have leveraged\n",
      "the power of designed LoRA initialization,\n",
      "to enhance the fine-tuning efficiency, or to\n",
      "preserve knowledge in the pre-trained LLM.\n",
      "However, none of these works can address\n",
      "the two cases at the same time. To this end,\n",
      "we intro\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/2505.23724v1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9810d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"../data/2505.23724v1.txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244178f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lexil\\AppData\\Local\\Temp\\ipykernel_49540\\2918464729.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(docs, embedding_model)\n",
    "faiss_index.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc03752",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=\"meta-llama/llama-4-scout:free\",\n",
    "    openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "wrapped_llm = LangChainLLM(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5d1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA stands for scale-aware Low-Rank Adaptation, which is an extension or a variation of the Low-Rank Adaptation (LoRA) method. LoRA is a technique used in machine learning, particularly in the context of large language models and other neural networks, to adapt or fine-tune these models efficiently on specific tasks or datasets.\n",
      "\n",
      "The primary differences between SC-LoRA and regular LoRA lie in their approach to adapting the model:\n",
      "\n",
      "1. **Scale Awareness**: SC-LoRA introduces a scale-aware mechanism. This means that SC-LoRA is designed to be aware of and adapt to different scales or magnitudes of the model parameters or the data. This can be particularly useful in scenarios where the input data or the model's parameters have a wide range of values, and a one-size-fits-all adaptation approach might not be optimal.\n",
      "\n",
      "2. **Dynamic Adaptation**: SC-LoRA might offer a more dynamic adaptation mechanism compared to regular LoRA. While LoRA adapts the model by learning low-rank updates to the model's weights, SC-LoRA could potentially adjust these updates based on the scale of the parameters or the input data. This could allow for more flexible and effective adaptations in certain scenarios.\n",
      "\n",
      "3. **Handling of Large Models**: Both SC-LoRA and LoRA are designed to work with large models, but SC-LoRA's scale-aware approach might provide better handling of models with a large range of parameter values. This could make SC-LoRA particularly suitable for models where some parameters or parts of the model have significantly larger or smaller values than others.\n",
      "\n",
      "4. **Performance and Efficiency**: The performance and computational efficiency of SC-LoRA compared to LoRA can vary depending on the specific application and dataset. SC-LoRA's additional scale-aware mechanism might offer better performance in certain tasks by more accurately adapting the model to the data, but it could also potentially increase the computational requirements.\n",
      "\n",
      "In summary, SC-LoRA differs from regular LoRA by incorporating a scale-aware mechanism that allows for more dynamic and potentially more effective adaptations of large models to specific tasks or datasets. The actual benefits of SC-LoRA over LoRA would depend on the specific use case and the characteristics of the model and data being used.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does SC-LoRA differ from regular LoRA?\"\n",
    "response = llm.invoke([query])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4916a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC-LoRA and LoRA (Low-Rank Adaptation) are both methods used for efficient fine-tuning of large pre-trained models, particularly in the context of adapting these models to specific tasks or datasets. While they share some similarities, there are key differences between them:\n",
      "\n",
      "1. **Basic Approach**:\n",
      "   - **LoRA**: LoRA is a method that adapts a pre-trained model by adding low-rank matrices to its weights. Specifically, for a given layer, it updates the weights by adding a low-rank matrix (often represented as the product of two smaller matrices) to the original weights. This approach allows for efficient adaptation with a relatively small number of additional parameters.\n",
      "   - **SC-LoRA (Structured Compression and Low-Rank Adaptation)**: SC-LoRA extends the basic LoRA approach by incorporating structured compression. Before applying low-rank adaptation, SC-LoRA first compresses the model by pruning or reducing the dimensionality of the weights in a structured manner. This compression step aims to reduce the model size and improve efficiency further. After compression, the low-rank adaptation is applied to the compressed model.\n",
      "\n",
      "2. **Efficiency and Performance**:\n",
      "   - **LoRA**: Offers a good balance between efficiency and performance. By adding low-rank matrices, it efficiently adapts the model with minimal additional parameters, making it suitable for deployment on devices with limited resources.\n",
      "   - **SC-LoRA**: SC-LoRA aims to push the efficiency envelope further by combining model compression with low-rank adaptation. This can lead to even more compact models and potentially better performance in certain scenarios, as the initial compression step might help remove redundant or less important weights, making the subsequent adaptation process more effective.\n",
      "\n",
      "3. **Adaptation Flexibility and Complexity**:\n",
      "   - **LoRA**: Relatively straightforward to implement and understand, with a focus on low-rank updates. This simplicity can be an advantage in terms of ease of implementation and integration with existing model architectures.\n",
      "   - **SC-LoRA**: Introduces an additional step of structured compression, which can add complexity in terms of implementation and hyperparameter tuning. However, this extra step can provide more flexibility in managing model size and computational requirements.\n",
      "\n",
      "4. **Use Cases**:\n",
      "   - **LoRA**: Suitable for a wide range of applications where efficient fine-tuning of large pre-trained models is needed, such as in natural language processing or computer vision tasks.\n",
      "   - **SC-LoRA**: Particularly appealing in scenarios where extreme model compression and efficiency are crucial, such as deploying models on edge devices with very limited resources or in applications where latency and model size need to be minimized.\n",
      "\n",
      "In summary, while both LoRA and SC-LoRA are designed for efficient adaptation of large pre-trained models, SC-LoRA adds an extra layer of structured compression before applying low-rank adaptation, aiming for even greater efficiency and compactness. The choice between them would depend on the specific requirements of the task at hand, including the need for model size reduction, computational efficiency, and adaptation performance.\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = faiss_index.similarity_search(query, k=3)\n",
    "\n",
    "context = \" \".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    f\"Context: {context}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=query)\n",
    "]\n",
    "\n",
    "contextual_response = llm.invoke([query])\n",
    "print(contextual_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb7931",
   "metadata": {},
   "source": [
    "### Part 2: LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baab436",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(docs, embedding_model)\n",
    "faiss_index.save_local(\"faiss_index\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
